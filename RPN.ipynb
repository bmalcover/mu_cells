{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPN\n",
    "\n",
    "Afegim a la U-Net una branca nova, la branca de *region proposal network (RPN)*.  Introduida per primer cop per la *faster rcnn* duu a terme dues tasques alhora, per una part refina tot un conjunt de <a hfre=\"https://www.termcat.cat/ca/cercaterm/bounding%20box?type=basic\">envolupants </a> i per l'altra indica quina és la probabilitat que cada un d'ells contengui un objecte.\n",
    "\n",
    "<img style=\"width:75%\" src=\"https://tryolabs.com/blog/images/blog/post-images/2018-01-18-faster-rcnn/rpn-conv-layers.63c5bf86.png\" />\n",
    "\n",
    "### Importam llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import cv2 \n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import numpy as np\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from numpy.random import seed\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras.layers as keras_layer\n",
    "\n",
    "# Llibraries pròpies\n",
    "from u_cells.u_cells.data import unet as u_data\n",
    "from u_cells.u_cells.data import rpn as rpn_data\n",
    "from u_cells.u_cells.model import unet as u_model\n",
    "from u_cells.u_cells.model import rpn as rpn_model\n",
    "from u_cells.u_cells.model import resnet as resnet_model\n",
    "from u_cells.u_cells.common import config as rpn_config\n",
    "from u_cells.u_cells.common import data as common_data\n",
    "from u_cells.u_cells.common import metrics as rpn_metrics\n",
    "\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuració\n",
    "\n",
    "Primerament cream un classe configuració per l'execusió i entrenament de la xarxa. En aquesta classe deixam els valors per defecte exceptuant els casos del nombre de classes, la mida de les ancores, les pases per època i el llindar mínim de confiança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_CLASS = False\n",
    "PYRAMID = False\n",
    "TRANSFER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellConfig(rpn_config.Config):\n",
    "    \"\"\"Configuration for training on the toy  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"cells\"\n",
    "    BATCH_SIZE = 3\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    BACKBONE_STRIDES = [4]\n",
    "    RPN_ANCHOR_SCALES = [32]\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    if MULTI_CLASS:\n",
    "        NUM_CLASSES = 1 + 3  # Background + 3 classes\n",
    "    else:\n",
    "        NUM_CLASSES = 1 + 1  # Background + 3 classes\n",
    "    \n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 100\n",
    "#     LEARNING_RATE = 3e-01\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0\n",
    "    PRED_THRESHOLD = 0.99999995\n",
    "    \n",
    "    IMAGE_SHAPE = [512, 512, 3]\n",
    "    \n",
    "    IMAGE_MAX_DIM = 512\n",
    "    IMAGE_MIN_DIM = 400\n",
    "    \n",
    "    COMBINE_FG = True\n",
    "\n",
    "\n",
    "config = CellConfig()\n",
    "config.IMAGE_SHAPE = np.array([512,512,3])\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament\n",
    "\n",
    "Per realitzar l'entrenament primerament cream dos generadors d'imatges. Els generadors en el cas de la *RPN* es creen en dos temps. Primerament cream objectes **Dataset**.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Definim un objecte Dataset. Anàlogament a la configuració, ja definida, és basa en herència de classes abstractes definides a les llibreries. Un detall important és que en el cas de la RPN les dades es formen a partir dels envolupants, enlloc de l'inrevés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDataset(rpn_data.Dataset):\n",
    "\n",
    "    def load_cell(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the Erithocites2 dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "        if MULTI_CLASS:\n",
    "            self.add_class(\"cell\", 1, \"ELONGATED\")\n",
    "            self.add_class(\"cell\", 2, \"CIRCULAR\")\n",
    "            self.add_class(\"cell\", 3, \"OTHER\")\n",
    "        else:\n",
    "            self.add_class(\"cell\", 1, \"cell\")\n",
    "\n",
    "        \n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Anottation following the format oof VIA\n",
    "        annotations = json.load(open(os.path.join(dataset_dir, \"via_region_data.json\")))\n",
    "        annotations = list(annotations.values())  # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            # Get the x, y coordinaets of points of the polygons that make up\n",
    "            # the outline of each object instance. These are stores in the\n",
    "            # shape_attributes (see json format above)\n",
    "            # The if condition is needed to support VIA versions 1.x and 2.x.\n",
    "            if type(a['regions']) is dict:\n",
    "                a['regions'] = a['regions'].values()\n",
    "                \n",
    "            aux = [(r['shape_attributes'], r['type'] + 1) for r in a['regions']]\n",
    "            \n",
    "            polygons, cells = list(zip(*aux))\n",
    "            \n",
    "            \n",
    "            if not MULTI_CLASS:\n",
    "                cells = np.ones([len(polygons)], dtype=np.int32)\n",
    "            else:\n",
    "                cells = np.asarray(cells)\n",
    "\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"cell\",\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons, cells = cells)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "        \n",
    "        Args:\n",
    "            image_id:\n",
    "        \n",
    "        Returns:\n",
    "            masks:  A bool array of shape [height, width, instance count] with\n",
    "                    one mask per instance.\n",
    "            class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        image_info = self.image_info[image_id]\n",
    "\n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        gt_class = []\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        return mask.astype(np.bool), info[\"cells\"]\n",
    "        \n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        return info[\"path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vegada definit el *dataset* cream dues instàncies, una per l'entrenament i l'altra per validació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset.\n",
    "dataset_train = CellDataset()\n",
    "dataset_train.load_cell(\"./in/bboxes_class/\", \"train\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = CellDataset()\n",
    "dataset_val.load_cell(\"./in/bboxes_class/\", \"val\")\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "augmentation = [  # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5),  # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.2),  # vertically flip 20% of all images\n",
    "        # crop images by -5% to 10% of their height/width\n",
    "        # sometimes(iaa.CropAndPad(\n",
    "        #     percent=(-0.05, 0.1),\n",
    "        #     pad_mode=ia.ALL,\n",
    "        #     pad_cval=(0, 255)\n",
    "        # )),\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "            # scale images to 80-120% of their size, individually per axis\n",
    "            translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "            # translate by -20 to +20 percent (per axis)\n",
    "            rotate=(-45, 45),  # rotate by -45 to +45 degrees\n",
    "            shear=(-16, 16),  # shear by -16 to +16 degrees\n",
    "            order=[0, 1],  # use nearest neighbour or bilinear interpolation (fast)\n",
    "            cval=0,  # if mode is constant, use a cval between 0 and 255\n",
    "            mode=ia.ALL\n",
    "            # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        )),\n",
    "        # execute 0 to 5 of the following (less important) augmenters per image\n",
    "        # don't execute all of them, as that would often be way too strong\n",
    "        iaa.SomeOf((0, 5),\n",
    "                   [\n",
    "                       iaa.OneOf([\n",
    "                           iaa.GaussianBlur((0, 3.0)),  # blur images with a sigma between 0 and 3.0\n",
    "                           iaa.AverageBlur(k=(2, 7)),\n",
    "                           # blur image using local means with kernel sizes between 2 and 7\n",
    "                           iaa.MedianBlur(k=(3, 11)),\n",
    "                           # blur image using local medians with kernel sizes between 2 and 7\n",
    "                       ]),\n",
    "                   ],\n",
    "                   random_order=True)]\n",
    "\n",
    "augmentation = iaa.Sequential(augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = rpn_data.DataGenerator(100, dataset_train, config, shuffle=False, augmentation=augmentation)\n",
    "val_generator = rpn_data.DataGenerator(100, dataset_val, config, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Backbone* dataset\n",
    "\n",
    "També cream un generador per la ``U-Net`` sense RPN per si volem fer un entrenament en dues fases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_u = u_data.DataGenerator(3, 540 // 3, './in/CellPose/train/img/*.png', (512, 512), 3,\n",
    "                                         u_data.DataFormat.MASK, mask_path='./in/CellPose/train/masks/*.png', \n",
    "                                         augmentation=None, background=False, rgb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construim el model\n",
    "\n",
    "### Backbone model\n",
    "\n",
    "Primerament construim la U-Net que emprarem com a *backbone* per el model RPN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_NET = True\n",
    "\n",
    "if U_NET:\n",
    "    backbone = u_model.UNet(input_size=(512, 512, 3), out_channel=1, batch_normalization=True, residual=True)\n",
    "\n",
    "    input_image, encoder, mask_out = backbone.build(n_filters=16, layer_depth=5, dilation_rate=1, last_activation=\"sigmoid\")\n",
    "    backbone.compile(loss_func = \"binary_crossentropy\", run_eagerly=True)\n",
    "else:\n",
    "    input_image = keras_layer.Input(self.__input_size, name=\"input_image\")\n",
    "    embedded_layer = resnet_model.resnet_graphs(input_image, 'resnet50')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFER:\n",
    "    backbone.train(train_generator_u, None, 5, 540 // 3, check_point_path=None, validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Múltiples entrades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFER:\n",
    "    for l in backbone.model.layers:\n",
    "        l.trainable = False\n",
    "\n",
    "    for l in list(encoder.values())[::-1][:4]: # Four inner layers\n",
    "        l.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if PYRAMID:\n",
    "    features = rpn_model.RPN.features_2_rpn(list(encoder.values())[::-1][:2], 256)\n",
    "else:\n",
    "    features = list(encoder.values())[-2]\n",
    "    features = keras_layer.Concatenate(axis=-1)([features, keras_layer.UpSampling2D(size=(2, 2))(list(encoder.values())[-1])])\n",
    "    features = keras_layer.Conv2D(256, (1,1))(features)\n",
    "    features = keras_layer.Concatenate(axis=-1)([(list(encoder.values())[-3]), keras_layer.UpSampling2D(size=(2,2))(features)])\n",
    "    features = keras_layer.Conv2D(256, (1,1))(features)\n",
    "#     features = keras_layer.Concatenate(axis=-1)([(list(encoder.values())[-4]), keras_layer.UpSampling2D(size=(2,2))(features)])\n",
    "#     features = keras_layer.Conv2D(256, (1,1))(features)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rpn = rpn_model.RPN(rpn_model.NeuralMode.TRAIN, (512, 512, 3), features, 256, mask_out, \n",
    "                    input_image, config)\n",
    "\n",
    "rpn.build()\n",
    "rpn.compile()\n",
    "rpn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenam el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rpn.train(train_generator=train_generator, val_generator=val_generator, epochs=10, check_point_path=\"./pesos2.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferència\n",
    "\n",
    "Per realitzar la inferència generam un nou model, amb el mode ``INFERENCE``. Una vegada creat hem de carregar els pesos des d'un fitxer, generat quan acabam l'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn = rpn_model.RPN(rpn_model.NeuralMode.INFERENCE, (512, 512, 3), features, 256, mask_out, \n",
    "                    input_image, config)\n",
    "\n",
    "rpn.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn.load_weights(\"./pesos2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in val_generator:\n",
    "    break\n",
    "masks, cls, bboxes = rpn.predict(t[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (cls.shape[1] == bboxes.shape[1]) and (cls.shape[1] == val_generator.anchors.shape[0]), \"Ancores i predicció diferents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicam les *deltas* als anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = [64, 0, 448, 512]\n",
    "ORG_IMG = [2352, 3136, 3]\n",
    "IMG_SHAPE = [512, 512, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_deltas = bboxes[0] * config.RPN_BBOX_STD_DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_to_img(window_position, img_shape, bboxes):\n",
    "    bboxes = np.copy(bboxes)\n",
    "    window_shape = window_position[3] - window_position[1], window_position[2] - window_position[0]\n",
    "\n",
    "    for i in range(bboxes.shape[1]):\n",
    "        # Coordinades_img = (Coordinades_window - origen) * (Widht_img / window_width)\n",
    "        bboxes[:, i] = (bboxes[:, i] - window_position[i % 2]) * (img_shape[(i + 1) % 2] / window_shape[ i % 2])\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_deltas = val_generator.decode_deltas(bboxes_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_deltas = windows_to_img(WINDOW, ORG_IMG, bboxes_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtram els envolupants amb una *objecteveness* menor que 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_img = cv2.imread(\"./in/bboxes_class/val/4.png\")\n",
    "in_img = in_img.astype(np.uint8)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(in_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_filtered = bboxes_deltas[cls[0][:, 1] > 0.7]\n",
    "bboxes_filtered = bboxes_filtered.astype(int)\n",
    "bboxes_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = np.copy(in_img.astype(np.uint8))\n",
    "\n",
    "for r in common_data.non_max_suppression_fast(bboxes_filtered, 0.3, cls[0][:, 1][cls[0][:, 1] > 0.7]):  \n",
    "    out_img = cv2.rectangle(out_img, (r[1], r[0]), (r[3], r[2]), (0, 255, 0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(out_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mètriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_g = []\n",
    "p_g = []\n",
    "\n",
    "dataset = dataset_val\n",
    "generador = val_generator\n",
    "for idx in tqdm(dataset.image_ids):\n",
    "    img, _, _ , gt_bbox, _ = rpn_data.DataGenerator.load_image_gt(dataset, config, idx)\n",
    "    img = generador.mold_image(img)\n",
    "    _, cls, bboxes = rpn.predict(img.reshape(1, 512, 512, 3))\n",
    "    \n",
    "    bboxes_deltas = bboxes[0] * config.RPN_BBOX_STD_DEV\n",
    "    bboxes_deltas = generador.decode_deltas(bboxes_deltas)\n",
    "    \n",
    "    bboxes_filtered = bboxes_deltas[cls[0][:, 1] > 0.7]\n",
    "    cls =  cls[0][:, 1][cls[0][:, 1] > 0.7]\n",
    "\n",
    "    inside_the_box = ((bboxes_filtered[:, 0] > WINDOW[0] + 5) & \n",
    "                       (bboxes_filtered[:, 1] > WINDOW[1] + 5) & \n",
    "                       (bboxes_filtered[:, 2] < WINDOW[2] - 5) & \n",
    "                       (bboxes_filtered[:, 3] < WINDOW[3] - 5))\n",
    "    \n",
    "    bboxes_filtered = bboxes_filtered[inside_the_box]\n",
    "    cls = cls[inside_the_box]\n",
    "    \n",
    "    bboxes_filtered = common_data.non_max_suppression_fast(bboxes_filtered, 0.3, cls)\n",
    "    \n",
    "    _, _, pred = rpn_metrics.relate_bbox_to_gt(bboxes_filtered, gt_bbox)\n",
    "\n",
    "    gt_p = [1] * len(pred)\n",
    "\n",
    "    if len(pred) < len(bboxes_filtered):\n",
    "        diff = len(bboxes_filtered) - len(pred)\n",
    "        pred = pred + [1] * diff\n",
    "        gt_p = gt_p + [0] * diff\n",
    "        \n",
    "    gt_g = gt_g + gt_p\n",
    "    p_g = p_g + pred\n",
    "    \n",
    "rpn_metrics.basic_metrics(gt_g, p_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"./2.png\", out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator.anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros_like(in_img)\n",
    "aux = zip(cls[0][:, 1], val_generator.anchors)\n",
    "aux = sorted(aux, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "for objecteveness, bounding_box in aux:\n",
    "#     print((bounding_box[3] - bounding_box[1]) * (bounding_box[2] - bounding_box[0]))\n",
    "    bounding_box = windows_to_img(WINDOW, ORG_IMG, np.array([bounding_box]))\n",
    "    bounding_box = bounding_box[0].astype(int)\n",
    "    \n",
    "    cv2.rectangle(weights, (bounding_box[1], bounding_box[0]), (bounding_box[3], bounding_box[2]), (255 * objecteveness, 0, 0), -1)\n",
    "\n",
    "dst = cv2.addWeighted(in_img, 0.5, weights, 0.5, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(dst);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
