{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPN\n",
    "\n",
    "Afegim a la U-Net una branca nova, la branca de *region proposal network (RPN)*.  Introduida per primer cop per la *faster rcnn* duu a terme dues tasques alhora, per una part refina tot un conjunt de <a hfre=\"https://www.termcat.cat/ca/cercaterm/bounding%20box?type=basic\">envolupants </a> i per l'altra indica quina és la probabilitat que cada un d'ells contengui un objecte.\n",
    "\n",
    "<img style=\"width:75%\" src=\"https://www.researchgate.net/publication/333048961/figure/fig1/AS:758094162296847@1557755141554/The-framework-of-Faster-R-CNN-RPN-region-proposal-network-RoI-region-of-interest-FC.ppm\" />\n",
    "\n",
    "### Importam llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import colorsys\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2 \n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from numpy.random import seed\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import patches,  lines\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras.layers as keras_layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils as KU\n",
    "\n",
    "# Llibraries pròpies\n",
    "from u_cells.u_cells.data import unet as u_data\n",
    "from u_cells.u_cells.data import rpn as rpn_data\n",
    "from u_cells.u_cells.data import datasets as rpn_datasets\n",
    "from u_cells.u_cells.model import unet as u_model\n",
    "from u_cells.u_cells.model import rpn as rpn_model\n",
    "from u_cells.u_cells.model import resnet as resnet_model\n",
    "from u_cells.u_cells.common import config as rpn_config\n",
    "from u_cells.u_cells.common import data as common_data\n",
    "from u_cells.u_cells import layers as own_layers\n",
    "from u_cells.u_cells.common import metrics as rpn_metrics\n",
    "from u_cells.u_cells.common import losses as rpn_losses\n",
    "\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Optimisation Flags - Do not remove\n",
    "# ============================================\n",
    "\n",
    "os.environ['CUDA_CACHE_DISABLE'] = '0'\n",
    "\n",
    "os.environ['HOROVOD_GPU_ALLREDUCE'] = 'NCCL'\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_GPU_THREAD_COUNT'] = '1'\n",
    "\n",
    "os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'\n",
    "\n",
    "os.environ['TF_ADJUST_HUE_FUSED'] = '1'\n",
    "os.environ['TF_ADJUST_SATURATION_FUSED'] = '1'\n",
    "os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "\n",
    "os.environ['TF_SYNC_ON_FINISH'] = '0'\n",
    "os.environ['TF_AUTOTUNE_THRESHOLD'] = '2'\n",
    "os.environ['TF_DISABLE_NVTX_RANGES'] = '1'\n",
    "\n",
    "# ================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_colors(N, bright=True):\n",
    "    \"\"\"\n",
    "    Generate random colors.\n",
    "    To get visually distinct colors, generate them in HSV space then\n",
    "    convert to RGB.\n",
    "    \"\"\"\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "def draw_bboxes(img, bboxes, thickness=3):\n",
    "    img = np.copy(img.astype(np.uint8))\n",
    "    colors = random_colors(len(bboxes))\n",
    "\n",
    "    for bbox, color in zip(bboxes, colors):\n",
    "        color = np.array(color) * 255\n",
    "        img = cv2.rectangle(img, (bbox[1], bbox[0]), (bbox[3], bbox[2]), color, thickness)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def make_masks(mask, slice_mask):\n",
    "    if isinstance(slice_mask, list):\n",
    "        if len(slice_mask) != 2:\n",
    "            raise ValueError\n",
    "        \n",
    "        mask = np.sum(mask[:,:, slice_mask[0]:slice_mask[1]], axis=-1)\n",
    "    elif isinstance(slice_mask, list):\n",
    "        mask = mask[:, :, slice_mask]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuració\n",
    "\n",
    "Primerament cream un classe configuració per l'execusió i entrenament de la xarxa. En aquesta classe deixam els valors per defecte exceptuant els casos del nombre de classes, la mida de les ancores, les pases per època i el llindar mínim de confiança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_CLASS = False\n",
    "PYRAMID = False\n",
    "TRANSFER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellConfig(rpn_config.Config):\n",
    "    \"\"\"Configuration for training on the toy  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"cells\"\n",
    "    BATCH_SIZE = 6\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    BACKBONE_STRIDES = [4]\n",
    "    RPN_ANCHOR_SCALES = [32]\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    if MULTI_CLASS:\n",
    "        NUM_CLASSES = 1 + 3  # Background + 3 classes\n",
    "    else:\n",
    "        NUM_CLASSES = 1 + 1  # Background + 3 classes\n",
    "    \n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 50\n",
    "#     LEARNING_RATE = 3e-01\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0\n",
    "    PRED_THRESHOLD = 0.99999995\n",
    "    \n",
    "    IMAGE_SHAPE = [512, 512, 3]\n",
    "    \n",
    "    IMAGE_MAX_DIM = 512\n",
    "    IMAGE_MIN_DIM = 400\n",
    "    \n",
    "    COMBINE_FG = False\n",
    "    RANDOM_MASKS = False\n",
    "    MAKE_BACKGROUND_MASK = False\n",
    "#     RPN_TRAIN_ANCHORS_PER_IMAGE = 200\n",
    "    VALIDATION_STEPS = 10\n",
    "    MAX_GT_INSTANCES = 100\n",
    "    \n",
    "    DO_MASK = True\n",
    "    DO_MASK_CLASS = True\n",
    "    DO_MERGE_BRANCH = True\n",
    "\n",
    "\n",
    "config = CellConfig()\n",
    "config.IMAGE_SHAPE = np.array([512,512,3])\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament\n",
    "\n",
    "Per realitzar l'entrenament primerament cream dos generadors d'imatges. Els generadors en el cas de la *RPN* es creen en dos temps. Primerament cream objectes **Dataset**.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Definim un objecte Dataset. Anàlogament a la configuració, ja definida, és basa en herència de classes abstractes definides a les llibreries. Un detall important és que en el cas de la RPN les dades es formen a partir dels envolupants, enlloc de l'inrevés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = rpn_datasets.ErithocytesDataset([(\"cell\", 1, \"cell\")], \"bboxes.json\")\n",
    "dataset_train.load_cell(\"./in/eritocitos_augmented/\", rpn_datasets.Subset.TRAIN)\n",
    "dataset_train.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset\n",
    "dataset_val = rpn_datasets.ErithocytesDataset([(\"cell\", 1, \"cell\")], \"bboxes.json\")\n",
    "dataset_val.load_cell(\"./in/eritocitos_augmented/\", rpn_datasets.Subset.VALIDATION)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vegada definit el *dataset* cream dues instàncies, una per l'entrenament i l'altra per validació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = rpn_data.DataGenerator(50, dataset_train, config, shuffle=False, phantom_output=True)\n",
    "val_generator = rpn_data.DataGenerator(2, dataset_val, config, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construim el model\n",
    "\n",
    "### Backbone model\n",
    "\n",
    "Primerament construim la U-Net que emprarem com a *backbone* per el model RPN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(mode : rpn_model.NeuralMode):\n",
    "    encoder = u_model.EncoderUNet(input_size=(512, 512, 3), residual = True)\n",
    "    input_image, embedded = encoder.build(n_filters=16, last_activation='softmax', dilation_rate=1, layer_depth=5)\n",
    "\n",
    "    features = list(encoder.layers.values())[-2]\n",
    "    features = keras_layer.Concatenate(axis=-1, name=\"conc_1\")([features, keras_layer.Conv2DTranspose(256, kernel_size=(3, 3), strides=(2, 2), name=\"convd_tranposed_1\", padding=\"same\")(list(encoder.layers.values())[-1])])\n",
    "    features = keras_layer.Conv2D(256, (1,1), name=\"conv_1\")(features)\n",
    "    features = keras_layer.Concatenate(axis=-1, name=\"conc_2\")([(list(encoder.layers.values())[-3]), keras_layer.Conv2DTranspose(256, name=\"convd_tranposed_2\", kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(features)])\n",
    "    features = keras_layer.Conv2D(256, (1,1), name=\"conv_2\")(features)\n",
    "\n",
    "    rpn = rpn_model.RPN(mode, (512, 512, 3), features, 256, None, input_image, config)\n",
    "    rpn_out, rpn_conv  = rpn.build_rpn(features)\n",
    "\n",
    "    _, rpn_class, rpn_bbox  = rpn_out\n",
    "\n",
    "    grad_cam = {}\n",
    "    coord_conv = {}\n",
    "    size = 512\n",
    "    for key_layer, value in list(encoder.layers.items()):\n",
    "#         grad_cam[key_layer] = rpn_layers.GradCAM(name=f\"grad_cam_{key_layer}\")(input_image, encoder.layers[key_layer], rpn_class)\n",
    "        grads = keras_layer.Lambda(lambda x: tf.gradients(x[1], x[0], unconnected_gradients='zero'), name=f\"grad_{key_layer}\")([encoder.layers[key_layer], rpn_class])\n",
    "\n",
    "        # This is a vector where each entry is the mean intensity of the gradient over a specific\n",
    "        # feature map channel\n",
    "        pooled_grads = keras_layer.Lambda(lambda x: tf.reduce_mean(x, axis=(1, 2)), name=f\"pooled_grads_{key_layer}\")(grads[0])\n",
    "\n",
    "        # We multiply each channel in the feature map array by \"how important this channel is\" with\n",
    "        # regard to the top predicted class then sum all the channels to obtain the heatmap class\n",
    "        # activation\n",
    "        last_conv_layer_output = encoder.layers[key_layer]\n",
    "\n",
    "        pooled_grads = keras_layer.Lambda(\n",
    "            lambda x: tf.expand_dims(tf.expand_dims(x, axis=1), axis=1))(pooled_grads)\n",
    "        heatmap = keras_layer.Lambda(lambda x: x[0] * x[1])([last_conv_layer_output, pooled_grads])\n",
    "        heatmap = keras_layer.Lambda(lambda x: tf.reduce_sum(x, axis=-1))(heatmap)\n",
    "        heatmap = keras_layer.Lambda(lambda x: tf.expand_dims(x, axis=-1))(heatmap)\n",
    "        \n",
    "        heatmap = keras_layer.Lambda(lambda x: tf.maximum(x, 0) / tf.math.reduce_max(x))(heatmap)\n",
    "        \n",
    "        coord_conv[key_layer] = [size, size]\n",
    "        size /= 2\n",
    "        grad_cam[key_layer] = heatmap\n",
    "        \n",
    "    decoder = u_model.DecoderUNet(input_size=None, residual=True, n_channels=100, class_output_size=512, \n",
    "                                  merge_branch=True)\n",
    "#     mask_out = decoder.build(n_filters=16, last_activation='sigmoid', extra_layer = grad_cam, encoder=encoder,\n",
    "    mask_out, class_out, merge_branch = decoder.build(n_filters=16, last_activation='sigmoid', encoder=encoder, \n",
    "                             dilation_rate=1, embedded=embedded)\n",
    "    \n",
    "    rpn.build(mask_shape = [512, 512, None], rpn=rpn_out, mask_output=mask_out, do_mask=True, mask_class=class_out, \n",
    "              merge_branch=merge_branch)\n",
    "    \n",
    "    return rpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rpn = build_model(rpn_model.NeuralMode.TRAIN)\n",
    "rpn.compile(do_mask=True, do_class_mask=True, do_merge_branch=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datetime_str = ('{date:%Y-%m-%d-%H:%M:%S}'.format(date=datetime.now()))\n",
    "print(datetime_str)\n",
    "rpn.train(train_generator=train_generator, val_generator=val_generator, epochs=100, check_point_path=\"./pesos.hdf5\",\n",
    "          validation_steps=2, \n",
    "          callbacks = [tf.keras.callbacks.TensorBoard(log_dir=f\"./out/logs/{datetime_str}\", histogram_freq = 1,\n",
    "                                                profile_batch = '500,520')])\n",
    "#           use_multiprocessing=True, workers=12) # To increase performance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.RPN_NUM_OUTPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenam el model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferència\n",
    "\n",
    "Per realitzar la inferència generam un nou model, amb el mode ``INFERENCE``. Una vegada creat hem de carregar els pesos des d'un fitxer, generat quan acabam l'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rpn = build_model(rpn_model.NeuralMode.INFERENCE)\n",
    "rpn.load_weights(\"./pesos.hdf5\")\n",
    "rpn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_generator:\n",
    "    break\n",
    "masks, cls, bboxes, msk_cls, mask_merge = rpn.predict(t[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_cls[0] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(25, 25))\n",
    "for i in range(81):\n",
    "#     print((masks[0, :, :, i].min(), masks[0, :, :, i].max()))\n",
    "    plt.subplot(9, 9, i + 1)\n",
    "    mask = masks[0,:,:,i]\n",
    "    mask[mask < 0.5] = 0 \n",
    "    plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "# m = masks[2, :, :, 0]\n",
    "# m[m < 0.5] = 0\n",
    "plt.imshow(mask_merge[0, :, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (cls.shape[1] == bboxes.shape[1]) and (cls.shape[1] == val_generator.anchors.shape[0]), \"Ancores i predicció diferents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicam les *deltas* als anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = [64, 0, 448, 512]\n",
    "ORG_IMG = [2352, 3136, 3]\n",
    "IMG_SHAPE = [512, 512, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_deltas = bboxes[1] * config.RPN_BBOX_STD_DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_to_img(window_position, img_shape, bboxes):\n",
    "    bboxes = np.copy(bboxes)\n",
    "    window_shape = window_position[3] - window_position[1], window_position[2] - window_position[0]\n",
    "\n",
    "    for i in range(bboxes.shape[1]):\n",
    "        # Coordinades_img = (Coordinades_window - origen) * (Widht_img / window_width)\n",
    "        bboxes[:, i] = (bboxes[:, i] - window_position[i % 2]) * (img_shape[(i + 1) % 2] / window_shape[ i % 2])\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_deltas = val_generator.decode_deltas(bboxes_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_deltas = windows_to_img(WINDOW, ORG_IMG, bboxes_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtram els envolupants amb una *objecteveness* menor que 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_img = cv2.imread(\"./in/bboxes_class/val/5.png\")\n",
    "in_img = in_img.astype(np.uint8)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(in_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_filtered = cls[1][:, 1][cls[1][:, 1] > 0.7]\n",
    "bboxes_filtered = bboxes_deltas[cls[1][:, 1] > 0.7]\n",
    "bboxes_filtered = bboxes_filtered.astype(int)\n",
    "\n",
    "cls_filtered = cls_filtered[(bboxes_filtered[:, 0] > 0) & (bboxes_filtered[:, 1] > 0) & (bboxes_filtered[:, 3] < 3136 ) & (bboxes_filtered[:, 2] < 2352)]\n",
    "bboxes_filtered = (bboxes_filtered[(bboxes_filtered[:, 0] > 0) & (bboxes_filtered[:, 1] > 0) & (bboxes_filtered[:, 3] < 3136) & (bboxes_filtered[:, 2] < 2352)])\n",
    "\n",
    "bboxes_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "bboxes_def = common_data.non_max_suppression_fast(bboxes_filtered, 0.3, cls_filtered)\n",
    "img = draw_bboxes(in_img, bboxes_def, 3)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_def.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mètriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6cc39500af44beb2c8b359c05be86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision BB</th>\n",
       "      <th>Recall BB</th>\n",
       "      <th>F1 BB</th>\n",
       "      <th>Diff MSK_CLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>9.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>9.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>15.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>5.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>5.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>9.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>5.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>7.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>12.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.747265</td>\n",
       "      <td>0.876765</td>\n",
       "      <td>0.806852</td>\n",
       "      <td>7.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Precision BB  Recall BB     F1 BB  Diff MSK_CLS\n",
       "0       0.782609   0.900000  0.837209        6.0000\n",
       "1       0.660377   0.945946  0.777778        6.0000\n",
       "2       0.627907   0.771429  0.692308        9.0000\n",
       "3       0.695652   0.969697  0.810127        6.0000\n",
       "4       0.627907   0.658537  0.642857        9.0000\n",
       "5       0.700000   0.933333  0.800000        8.0000\n",
       "6       0.733333   0.897959  0.807339        2.0000\n",
       "7       0.655738   0.769231  0.707965       15.0000\n",
       "8       0.765432   0.873239  0.815789        5.0000\n",
       "9       0.732394   0.825397  0.776119        5.0000\n",
       "10      0.925373   0.939394  0.932331        9.0000\n",
       "11      0.885714   0.984127  0.932331        5.0000\n",
       "12      0.762712   0.957447  0.849057        7.0000\n",
       "13      0.652174   0.769231  0.705882       12.0000\n",
       "14      0.806452   0.925926  0.862069        6.0000\n",
       "15      0.804348   0.840909  0.822222        3.0000\n",
       "16      0.747265   0.876765  0.806852        7.0625"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THRESH = 0.7\n",
    "gt_g = []\n",
    "p_g = []\n",
    "\n",
    "dataset = dataset_val\n",
    "generador = val_generator\n",
    "resultats = []\n",
    "diff_gen = 0\n",
    "for idx in tqdm(dataset.image_ids):\n",
    "    org_img, _, _ , gt_bbox, mask_gt = rpn_data.DataGenerator.load_image_gt(dataset, config, idx)\n",
    "    img = np.copy(org_img)\n",
    "    \n",
    "    img = generador.mold_image(img)\n",
    "    mask_pred, cls, bboxes, msk_cls, mask_merge = rpn.predict(img.reshape(1, 512, 512, 3))\n",
    "    \n",
    "    bboxes_deltas = bboxes[0] * config.RPN_BBOX_STD_DEV\n",
    "    bboxes_deltas = generador.decode_deltas(bboxes_deltas)\n",
    "    \n",
    "    bboxes_filtered = bboxes_deltas[cls[0][:, 1] > THRESH]\n",
    "    cls =  cls[0][:, 1][cls[0][:, 1] > THRESH]\n",
    "\n",
    "    inside_the_box = ((bboxes_filtered[:, 0] > WINDOW[0] + 5) & \n",
    "                       (bboxes_filtered[:, 1] > WINDOW[1] + 5) & \n",
    "                       (bboxes_filtered[:, 2] < WINDOW[2] - 5) & \n",
    "                       (bboxes_filtered[:, 3] < WINDOW[3] - 5))\n",
    "    \n",
    "    bboxes_filtered = bboxes_filtered[inside_the_box]\n",
    "    cls = cls[inside_the_box]\n",
    "    \n",
    "    bboxes_filtered = common_data.non_max_suppression_fast(bboxes_filtered, 0.3, cls)\n",
    "    res = draw_bboxes(org_img, bboxes_filtered, 1)\n",
    "    \n",
    "    img_path = os.path.join(\".\", \"out\", \"res\")\n",
    "    os.makedirs(img_path, exist_ok=True)\n",
    "    cv2.imwrite(os.path.join(img_path, f\"{idx}.png\"), res)\n",
    "    \n",
    "    _, _, pred = rpn_metrics.relate_bbox_to_gt(bboxes_filtered, gt_bbox)\n",
    "\n",
    "    gt_p = [1] * len(pred)\n",
    "\n",
    "    if len(pred) < len(bboxes_filtered):\n",
    "        diff = len(bboxes_filtered) - len(pred)\n",
    "        pred = pred + [1] * diff\n",
    "        gt_p = gt_p + [0] * diff\n",
    "        \n",
    "    gt_g = gt_g + gt_p\n",
    "    p_g = p_g + pred\n",
    "    metrics = list(rpn_metrics.basic_metrics(gt_p, pred))\n",
    "    \n",
    "    msk_cls[msk_cls < 0.5] = 0\n",
    "    diff = np.abs(mask_gt.shape[-1] - np.count_nonzero(msk_cls))\n",
    "    diff_gen += diff\n",
    "    \n",
    "    metrics.append(diff)\n",
    "#     rpn_losses.onw_dice_coefficient()\n",
    "    \n",
    "    resultats.append(metrics)\n",
    "\n",
    "diff_gen /= len(dataset.image_ids)\n",
    "resultats.append(list(rpn_metrics.basic_metrics(gt_g, p_g)) + [diff_gen])\n",
    "df = pd.DataFrame(resultats)\n",
    "df.columns = ['Precision BB', 'Recall BB', 'F1 BB', \"Diff MSK_CLS\"]\n",
    "\n",
    "# df.to_csv(os.path.join(img_path, \"resultats_experiment_.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(img_path, \"resultats_experiment_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rpn = rpn_model.RPN(rpn_model.NeuralMode.TRAIN, (512, 512, 3), features, 256, mask_out, \n",
    "                    input_image, config)\n",
    "\n",
    "rpn.build()\n",
    "rpn.compile()\n",
    "rpn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Múltiples entrades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFER:\n",
    "    for l in backbone.model.layers:\n",
    "        l.trainable = False\n",
    "\n",
    "    for l in list(encoder.values())[::-1][:4]: # Four inner layers\n",
    "        l.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if PYRAMID:\n",
    "    features = rpn_model.RPN.features_2_rpn(list(encoder.values())[::-1][:3], 256)\n",
    "    features = features[::-1]\n",
    "else:\n",
    "    feat = []\n",
    "    features = list(encoder.values())[-2]\n",
    "    features = keras_layer.Concatenate(axis=-1)([features, keras_layer.UpSampling2D(size=(2, 2))(list(encoder.values())[-1])])\n",
    "    features = keras_layer.Conv2D(256, (1,1))(features)\n",
    "    features = keras_layer.Concatenate(axis=-1)([(list(encoder.values())[-3]), keras_layer.UpSampling2D(size=(2,2))(features)])\n",
    "    features = keras_layer.Conv2D(256, (1,1))(features)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn = rpn_model.RPN(rpn_model.NeuralMode.INFERENCE, (512, 512, 3), features, 256, mask_out, \n",
    "                    input_image, config)\n",
    "\n",
    "rpn.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn.load_weights(\"./pesos2.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
